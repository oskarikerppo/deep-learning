{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwm2STsij5Mj"
      },
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaZgCX8xj5Mn"
      },
      "source": [
        "Let's implement a small network to play with. The network has input layer of 128 units, a hidden layer of 16 units and a sigmoid output unit. We collect the activations $Wx + b$ into vectors $a$ and the results of activation functions (applied on these activations) into vectors $h$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr1ouWvnj5Mo"
      },
      "source": [
        "N = 128  # Input size\n",
        "H = 16   # Hidden layer size\n",
        "M = 1    # Output size\n",
        "\n",
        "x_input = np.random.randn(N)\n",
        "\n",
        "# first affine layer weights and biases\n",
        "W1 = np.random.randn(N, H)\n",
        "b1 = 0.05 * np.random.randn(H)\n",
        "# second affine layer weights and biases\n",
        "W2 = np.random.randn(H, M)\n",
        "b2 = 0.05 * np.random.randn(M)\n",
        "\n",
        "# In the example case there's actually no need to store the values a but here they are anyway\n",
        "a1 = np.zeros(H)            # W1 dot x + b1\n",
        "h1 = np.zeros(H)            # f(a1) (f = sigmoid in the following code)\n",
        "a2 = np.zeros(M)            # W2 dot h1 + b2\n",
        "h2 = np.zeros(M)            # f(a2)\n",
        "\n",
        "dW1 = np.zeros((N, H))      # gradient for W1 etc.\n",
        "db1 = np.zeros(H)\n",
        "dW2 = np.zeros((N, M))\n",
        "db2 = np.zeros(M)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXsLE_SwoSwK"
      },
      "source": [
        "Forward pass of a layer (pre-activation function value)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMjgUVKij5Mq"
      },
      "source": [
        "def affine_forward(x, W, b):\n",
        "    out = np.dot(x, W) + b\n",
        "    return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGs-e3JRj5Ms"
      },
      "source": [
        "The backward pass returns gradients `g_x`, `g_W` and `g_b`. The last two contain the values gradient descent uses in its updates. Gradient `g_x` is needed to proceed further in the network (it will be the `g_upstream` for the previous layer etc.).\n",
        "\n",
        "The implementation consists of Numpy code for the chain rule applications.  Since `b`is not needed to compute its gradient (or any other gradients) it's not passed as an argument either."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNqHkW04j5Mt"
      },
      "source": [
        "def affine_backward(g_upstream, x, W):\n",
        "    g_b = np.copy(g_upstream)\n",
        "    g_W = np.outer(x, g_upstream)\n",
        "    g_x = np.dot(g_upstream, W.T)\n",
        "    return g_x, g_W, g_b"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFpkib_8j5Mv"
      },
      "source": [
        "Couple of element-wise nonlinearities along with their backward passes. Recall that we store the values of these in variables $h$ while computing the forward pass. Hence, we can use them again while doing the backwards pass (to compute the gradient values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-yjGNjuj5Mw"
      },
      "source": [
        "def relu_forward(x):\n",
        "    out = np.maximum(np.zeros(x.shape), x)\n",
        "    return out\n",
        "\n",
        "# h = layer output in forward pass\n",
        "# since h comes from relu, we know that the partial derivatives are 1 for h[i] > 0, and 0 otherwise\n",
        "def relu_backward(g_upstream, h):\n",
        "    # np.where creates an array of ones and zeros (depending on the outcome of the given test)\n",
        "    # multiplication * is the element-wise multiplication of two vectors\n",
        "    # this may look a bit awkward but allows the use of vector operations in all computations\n",
        "    g_x = g_upstream * np.where(h > 0, np.ones(h.shape), np.zeros(h.shape))\n",
        "    return g_x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHBRTyrMj5My"
      },
      "source": [
        "def sigmoid_forward(x):\n",
        "    s = 1. / (1. + np.exp(-x))\n",
        "    return s\n",
        "\n",
        "# h = sigmoid output in forward pass\n",
        "def sigmoid_backward(g_upstream, h):\n",
        "    # elementwise products\n",
        "    sig_g = h * (1. - h)\n",
        "    return g_upstream * sig_g"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-fYBkbWj5M1"
      },
      "source": [
        "Loss function (binary cross-entropy) and its differential. Note that the differential has no parameter for the \"upstream gradient\" simply because there is none when this function is used (you can think it's 1). This gives us the first upstream gradient to start from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCwwzfvUj5M1"
      },
      "source": [
        "# We add a small number to divisors to avoid div by zero\n",
        "# This is a standard trick in numeric computations involving divisions\n",
        "my_eps = 1e-8\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    if (y_true == 1):\n",
        "        return - math.log(y_pred)\n",
        "    else:\n",
        "        return - math.log(1.0 - y_pred)\n",
        "\n",
        "def cross_entropy_diff(y_true, y_pred):\n",
        "    if (y_true == 1):\n",
        "        my_divisor = y_pred\n",
        "    else:\n",
        "        my_divisor = (1.0 - y_pred)\n",
        "    my_divisor += my_eps\n",
        "    g_loss = -(1./my_divisor)\n",
        "    return g_loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCHCiEHgj5M3"
      },
      "source": [
        "The forward and backward passes are specialized to our small 2-layer network. They should be generalized to handle any number of layers. Also, the weights, biases and activation functions of each layer should be passed in as arguments (e.g. as components of some larger structure). The programmer has just been busy (lazy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8HzWlHZj5M4"
      },
      "source": [
        "# Using sigmoids all over\n",
        "# You can try the same using relus in the hidden layer\n",
        "def forward_pass(x):\n",
        "    a1 = affine_forward(x, W1, b1)\n",
        "    h1 = sigmoid_forward(a1) \n",
        "    a2 = affine_forward(h1, W2, b2)\n",
        "    h2 = sigmoid_forward(a2)\n",
        "    return (a1, h1, a2, h2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXfmOW7BDgrG"
      },
      "source": [
        "Backward pass starts from the cost computation (single element). It should be straightforward to extend it to a minibatch of elements (sum up and average). Similarly, this routine should be generalized for arbitrary (layered) networks, and the variables `h` and `W` should be passed in as parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlmD5Jo6j5M6"
      },
      "source": [
        "def backward_pass(y_true, y_pred):\n",
        "    g_loss = cross_entropy_diff(y_true, y_pred[0]) # forward pass result (h2) is a single-element array\n",
        "    # the initial upstream gradient\n",
        "    g = np.array([g_loss])\n",
        "\n",
        "    g = sigmoid_backward(g, h2) # using here h2 instead of y_pred so it looks more \"general\"\n",
        "    # print('g after passing layer 2 activation:', g)\n",
        "    g, dW2, db2 = affine_backward(g, h1, W2)\n",
        "    # print('g after passing layer 2:', g)\n",
        "    g = sigmoid_backward(g, h1)\n",
        "    # print('g after passing layer 1 activation:', g)\n",
        "    g, dW1, db1 = affine_backward(g, x_input, W1)\n",
        "    # print('g after passing layer 1:', g)\n",
        "    return (dW1, db1, dW2, db2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHkqArDUj5M8"
      },
      "source": [
        "One could also make the GD loop below into a routine of its own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgYbN-saj5M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31be340c-04de-492d-c9d2-85d50da5a9a5"
      },
      "source": [
        "# GD loop\n",
        "# one can execute this code cell several times to continue training\n",
        "n_iters = 1000\n",
        "# This is what our model should learn (from one random input)\n",
        "y_true = 1.\n",
        "lr = 0.01\n",
        "first_iter = True\n",
        "\n",
        "print(\"Training for\", n_iters, \"loops\")\n",
        "for i in range(n_iters):\n",
        "    a1, h1, a2, h2 = forward_pass(x_input)\n",
        "    # You could add similar print after each 100 epochs etc.\n",
        "    # Or make the reporting frequency a parameter\n",
        "    if first_iter:\n",
        "        print(\"Model output before training = \", h2)\n",
        "        loss = cross_entropy(y_true, h2[0])\n",
        "        print(\"Loss before training =         \", loss)\n",
        "        first_iter = False\n",
        "        \n",
        "    dW1, db1, dW2, db2 = backward_pass(1., h2)\n",
        "\n",
        "    W1 -= (lr * dW1)\n",
        "    b1 -= (lr * db1)\n",
        "    W2 -= (lr * dW2)\n",
        "    b2 -= (lr * db2)\n",
        "    \n",
        "print(\"Model output after training = \", h2)\n",
        "loss = cross_entropy(y_true, h2[0])\n",
        "print(\"Loss after training =         \", loss)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 loops\n",
            "Model output before training =  [0.05835468]\n",
            "Loss before training =          2.841215739184117\n",
            "Model output after training =  [0.98809613]\n",
            "Loss after training =          0.011975283772418993\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}